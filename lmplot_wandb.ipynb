{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Sample feather file\n",
        "!gdown 1JbWFWAMonwpQjVleR5UIxFpQqmtdPTjk"
      ],
      "metadata": {
        "id": "y327JTUGHT7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb"
      ],
      "metadata": {
        "id": "IrTgknTUHV30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage\n",
        "\n",
        "1. Select any of the model, task or metric as (str or list). If any of these is None, all will selected. (Default: None)\n",
        "2. Can also avoid writing complete task names. task=\"math\" will plot all tasks containing keyword \"math\".\n",
        "```\n",
        "lmp.lineplot(x=\"step\", model=[\"1.3B_deduped\", \"1.3B\"], task=\"math\", metric=\"acc\", compare=True,\n",
        "                project=\"my-project\", name=\"my-run\") # wandb args\n",
        "```\n",
        "3. Set, compare=True (default: False) if you want to compare models across different tasks and metrics.    \n",
        "Otherwise, seperate model plots will be logged.\n",
        "4. Get filtered dataframe for any model, task and metric.\n",
        "    ```\n",
        "  lmp.filter_df(model=[\"19M\", \"19M_dedup\"], , task=\"math\", metric=\"acc\", save_csv=\"19M.csv\")\n",
        "    ```\n",
        "5. Get raw dataframe\n",
        "  ```\n",
        "  lmp.get_df()\n",
        "  ```"
      ],
      "metadata": {
        "id": "WcffZ-uuJlnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gtlLgxofFqWV"
      },
      "outputs": [],
      "source": [
        "#@title Run Code\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "\n",
        "def read_feather(file_path):\n",
        "    \"\"\"\n",
        "    Read a feather file into a data frame.\n",
        "\n",
        "    Args:\n",
        "        file_path: path to the feather file.\n",
        "\n",
        "    Returns:\n",
        "        lmplot object.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_feather(file_path)\n",
        "    return lmplot(df)\n",
        "\n",
        "\n",
        "def collect(pathname, save_feather=None):\n",
        "    \"\"\"\n",
        "    Combine multiple eval_results.json files into a single data frame.\n",
        "\n",
        "    Args:\n",
        "        pathname: string\n",
        "                Pathname to search for eval_results.json files.\n",
        "        save_feather: string\n",
        "                path to save the data frame as a feather file.\n",
        "\n",
        "    Returns:\n",
        "        lmplot object.\n",
        "    \"\"\"\n",
        "\n",
        "    rgx_file_name = re.compile(\"^(?:(.*)_)?eval_results_([0-9-]+).json$\")\n",
        "\n",
        "    dict_list = []\n",
        "    for file_path in glob.glob(pathname):\n",
        "        file_name = os.path.basename(file_path)\n",
        "        m = rgx_file_name.match(file_name)\n",
        "        if m is None:\n",
        "            print(\"WARNING: cannot parse results file name '{}'\".format(file_name))\n",
        "            continue\n",
        "\n",
        "        header = dict()\n",
        "\n",
        "        run_id = m[1]\n",
        "        header[\"path\"] = file_name\n",
        "        header[\"timestamp\"] = m[2]\n",
        "\n",
        "        # Parse the file name and add\n",
        "        rgx_meta = re.compile(\"^(.*)-global_step(\\d+)$\")\n",
        "        rgx_m = rgx_meta.match(run_id)\n",
        "        if rgx_m is not None:\n",
        "            metadata_run_id = {\"model\": rgx_m[1], \"step\": int(rgx_m[2])}\n",
        "        else:\n",
        "            metadata_run_id = None\n",
        "\n",
        "        if metadata_run_id is None:\n",
        "            continue\n",
        "\n",
        "        # Read the json file into a data frame\n",
        "        with open(file_path) as f:\n",
        "            try:\n",
        "                eval_json = json.load(f)\n",
        "                result_json = eval_json[\"results\"]\n",
        "            except:\n",
        "                print(\n",
        "                    \"WARNING: cannot load file '{}'\".format(file_path), file=sys.stderr\n",
        "                )\n",
        "                continue\n",
        "\n",
        "        model = os.path.basename(eval_json[\"config\"][\"model_args\"][\"load\"])\n",
        "        metadata_config = {\"model\": model}\n",
        "\n",
        "        for task in result_json.keys():\n",
        "            for metric in result_json[task]:\n",
        "                record = header.copy()\n",
        "                record[\"task\"] = task\n",
        "                record[\"metric\"] = metric\n",
        "                record[\"value\"] = result_json[task][metric]\n",
        "                record.update(metadata_run_id)\n",
        "                record.update(metadata_config)\n",
        "\n",
        "                dict_list.append(record)\n",
        "\n",
        "    df = pd.json_normalize(dict_list)\n",
        "    columns = [(\"model\", str), (\"step\", int)]\n",
        "    for name, dtype in columns:\n",
        "        if name in df:\n",
        "            df[name] = df[name].astype(dtype)\n",
        "\n",
        "    if save_feather:\n",
        "        df.to_feather(save_feather)\n",
        "\n",
        "    return lmplot(df)\n",
        "\n",
        "\n",
        "class lmplot:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def get_df(self):\n",
        "        return self.df\n",
        "\n",
        "    def all_models(self):\n",
        "        \"\"\"\n",
        "        Return a list of all models in the data frame.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.df[\"model\"].unique().tolist()\n",
        "\n",
        "    def filter_df(self, model=None, task=None, metric=None, to_csv=None):\n",
        "        \"\"\"\n",
        "        Filter the data frame for the specified model, task, metric.\n",
        "\n",
        "        Args:\n",
        "            model: string or list of strings or None\n",
        "                If specified, only return info for the specified model(s).\n",
        "            task: string or list of strings or None\n",
        "                If specified, only return info for the specified task(s).\n",
        "            metric: string or list of strings or None\n",
        "                If specified, only return info for the specified metric(s).\n",
        "\n",
        "        Returns:\n",
        "            Dataframe with columns model, task, metric.\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.df\n",
        "\n",
        "        filters = {}\n",
        "        filters[\"model\"] = model if isinstance(model, list) else [model]\n",
        "        filters[\"task\"] = task if isinstance(task, list) else [task]\n",
        "        filters[\"metric\"] = metric if isinstance(metric, list) else [metric]\n",
        "\n",
        "        for colname, colvalues in filters.items():\n",
        "\n",
        "            if task is None and colname == \"task\":\n",
        "                # If task is not specified, we want to plot all tasks\n",
        "                colvalues = df[\"task\"].unique().tolist()\n",
        "\n",
        "            if metric is None and colname == \"metric\":\n",
        "                # If metric is not specified, we want to plot all metrics\n",
        "                colvalues = df[\"metric\"].unique().tolist()\n",
        "\n",
        "            invalid_colvalues = [\n",
        "                c for c in colvalues if c not in df[colname].unique().tolist()\n",
        "            ]\n",
        "\n",
        "            if invalid_colvalues:\n",
        "\n",
        "                if colname == \"task\":\n",
        "                    # Filter out tasks that are substrings of other tasks. Example: task=\"math\" will get all tasks containing \"math\".\n",
        "\n",
        "                    for invalid_task in invalid_colvalues:\n",
        "                        invalid_flag = True\n",
        "\n",
        "                        for task in df[\"task\"].unique().tolist():\n",
        "                            if invalid_task in task:\n",
        "                                colvalues.append(task)\n",
        "                                invalid_flag = False\n",
        "\n",
        "                        if invalid_flag:\n",
        "                            raise ValueError(\n",
        "                                f\"{colname} names {invalid_task} not found. Available are {df[colname].unique().tolist()}\"\n",
        "                            )\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        f\"{colname} names {invalid_colvalues} not found. Available are {df[colname].unique().tolist()}\"\n",
        "                    )\n",
        "\n",
        "            df = df[df[colname].isin(colvalues)]\n",
        "\n",
        "        df[\"step\"] = df[\"step\"].astype(int)\n",
        "        df = df.reset_index(drop=True)\n",
        "\n",
        "        if to_csv:\n",
        "            df.to_csv(to_csv, index=False)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _lineplot_tasks(\n",
        "        self,\n",
        "        df,\n",
        "        x=\"step\",\n",
        "        model=None,\n",
        "        task=None,\n",
        "        metric=None,\n",
        "        hue=\"model\",\n",
        "        compare=False,\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        Filter the dataframe and plot the lineplot for each.\n",
        "        \"\"\"\n",
        "\n",
        "        for task in df[\"task\"].unique().tolist():\n",
        "            task = str(task)\n",
        "            task_df = df[df[\"task\"] == task]\n",
        "            task_metrics = task_df[\"metric\"].unique().tolist()\n",
        "\n",
        "            for metric in task_metrics:\n",
        "                metric = str(metric)\n",
        "\n",
        "                metric_df = task_df[task_df[\"metric\"] == metric]\n",
        "\n",
        "                metric_df = metric_df.sort_values(by=x)\n",
        "\n",
        "                table = wandb.Table(dataframe=metric_df)\n",
        "                fields = {\n",
        "                    \"x-axis\": f\"{x}\",\n",
        "                    \"y-axis\": \"value\",\n",
        "                    \"color\": f\"{hue}\",\n",
        "                    \"metric\": f\"{metric}\",\n",
        "                    \"title\": f\"{task} ({metric})\",\n",
        "                }\n",
        "\n",
        "                custom_chart = wandb.plot_table(\n",
        "                    vega_spec_name=\"satpalsr/multiplot\", data_table=table, fields=fields\n",
        "                )\n",
        "\n",
        "                if compare:\n",
        "\n",
        "                    if len(task_metrics) == 1:\n",
        "                        wandb.log({f\"{task} {metric}\": custom_chart})\n",
        "                    else:\n",
        "                        wandb.log({f\"{task}/{metric}\": custom_chart})  # Add task panel\n",
        "\n",
        "                else:  # Add model panel\n",
        "\n",
        "                    wandb.log({f\"{model}/{task} ({metric})\": custom_chart})\n",
        "\n",
        "    def lineplot(\n",
        "        self,\n",
        "        x=\"step\",\n",
        "        model=None,\n",
        "        task=None,\n",
        "        metric=None,\n",
        "        hue=\"model\",\n",
        "        compare=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        Draw lineplot for each model, task and metric combination.\n",
        "\n",
        "        Args:\n",
        "            x (str): x-axis column name.\n",
        "            model (str or list or None): List of model names.\n",
        "            task (str or list or None): List of task names.\n",
        "            metric (str or list or None): List of metric names.\n",
        "            hue (str): Column name for hue\n",
        "            compare (bool): If True, Models are compared in each plot. Plots are saved in task folders.\n",
        "                            Else, Models are not compared. Plots are saved in model folders.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        if model is None:\n",
        "            model = self.all_models()\n",
        "\n",
        "        df = self.filter_df(model, task, metric)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"Model, task, metric combination not found.\")\n",
        "            return None\n",
        "\n",
        "        run = wandb.init(**kwargs)\n",
        "\n",
        "        if compare:\n",
        "            self._lineplot_tasks(df, x, model, task, metric, hue, compare)\n",
        "\n",
        "            run.finish()\n",
        "\n",
        "        else:\n",
        "\n",
        "            for model in df[\"model\"].unique().tolist():\n",
        "                model_df = df[df[\"model\"] == model]\n",
        "\n",
        "                self._lineplot_tasks(model_df, x, model, task, metric, hue, compare)\n",
        "\n",
        "            run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lmp = read_feather('data.feather')\n",
        "print(lmp.all_models())\n",
        "\n",
        "# or\n",
        "# lmp = collect(\"pythia/results/json/*/*.json\", save_feather='data.feather')"
      ],
      "metadata": {
        "id": "XGQd_5ZqHNgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run visuals change based on compare value and whether single or multiple metrics are provided. Run below 3 cells to see."
      ],
      "metadata": {
        "id": "YthJM7SAQWBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmp.lineplot(x='step',model=[\"1.3B_deduped\", \"1.3B\"], task='math', metric='acc', compare=True, project=\"pythia\", name=\"my-run-sm\") # Single Metric"
      ],
      "metadata": {
        "id": "oz6zDXo_O8BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lmp.lineplot(x='step',model=[\"1.3B_deduped\", \"1.3B\"], task='math', compare=True, project=\"pythia\", name=\"my-run-am\") # All Metrics"
      ],
      "metadata": {
        "id": "LEO6M_sDPYyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lmp.lineplot(x='step',model=[\"1.3B_deduped\", \"1.3B\"], task='math', project=\"pythia\", name=\"my-run-cf\") # compare = False"
      ],
      "metadata": {
        "id": "Z1Hb6lDTHgod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep any of the model, task or metric as None. (Default: None)\n",
        "lmp.lineplot(x='step',model=[\"1.3B_deduped\", \"1.3B\"], compare=True, project=\"pythia\", name=\"my-run-models\")"
      ],
      "metadata": {
        "id": "sahIdjvYQvvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}